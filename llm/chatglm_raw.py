import re
import json
import logging
from typing import (
    Any,
    Dict,
    List,
    Optional,
)

import requests
from termcolor import colored

import urllib3
urllib3.disable_warnings()

logger = logging.getLogger(__name__)


class _ChatGLMEndpointClient:
    """An API client that talks to a ChatGLM llm endpoint. 与 ChatGLM llm 端点对话的 API 客户端"""

    api_url: str = "https://117.50.198.214:8443/v1/chat/completions"
    headers: Dict = {
        'Host': 'api.glm.ai',
        'Authorization': 'Bearer ttEwtN5KYDKs97gQsIGEMMdV8TpvwGfPwRDfNPXhtxRh7f7pyfDUYeeXfgLZ7SJ2'
    }
    
    def post(self, request: Any) -> Any:
        retries = 3
        for _ in range(retries):
            response = requests.post(self.api_url, data=json.dumps(request), headers=self.headers, verify=False)
            response.encoding = 'UTF-8'
            try:
                choice = response.json()['choices'][0]
                if choice['finish_reason'] != 'stop':
                    print(f"Finish reason: {choice['finish_reason']}")
                    raise NotImplementedError
                return response.json()
            except requests.exceptions.JSONDecodeError:
                print(f"Response content: {response.text}")
        raise NotImplementedError

class ChatGLM:
    """Wrapper around ChatGLM large language models.
    To use, you should have the environment variable
    ``ChatGLM_API_KEY`` and ``ChatGLM_GROUP_ID`` set with your API key,
    or pass them as a named parameter to the constructor.
    围绕 ChatGLM 大型语言模型的 rapper。
    使用时，您需要在环境变量ChatGLM_API_KEY`` 和 `ChatGLM_GROUP_ID`` 设置为您的 API 密钥、或将它们作为命名参数传递给构造函数。
    Example:
     .. code-block:: python
         from langchain.llms.ChatGLM import ChatGLM
         ChatGLM = ChatGLM(model="<model_name>", ChatGLM_api_key="my-api-key",
          ChatGLM_group_id="my-group-id")
    示例：
     代码块:: python
         from langchain.llms.ChatGLM import ChatGLM
         ChatGLM = ChatGLM(model="<model_name>", ChatGLM_api_key="my-api-key"、
          ChatGLM_group_id="my-group-id")
    """

    @property
    def _default_params(self) -> Dict[str, Any]:
        return {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "stream": False,
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "ChatGLM"

    def __init__(self, **data: Any):
        super().__init__(**data)
        self.model: str = "glm-4-public"
        # self.model: str = "chatglm3-32b-v0.6-added"
        # self.model: str = "chatglm3-32b-v0.8-dpo-240102"
        # self.model: str = "chatglm3-32b-v0.9"
        self.max_tokens: int = 2048
        self.client: _ChatGLMEndpointClient = _ChatGLMEndpointClient()
    
    def generate(self, prompt: str):
        r"""Call out to ChatGLM's completion endpoint to chat
        Args:
            prompt: The prompt to pass into the model.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                response = ChatGLM("Tell me a joke.")
        """
        request = self._default_params
        
        # print(colored("Prompt: " + json.dumps([prompt]), 'red'))
        request["messages"] = [{
            "role": "user",
            "content": prompt
        }]
        response = self.client.post(request)
        response = response['choices'][0]['message']['content'].rstrip()
        return response

    # 
    def invoke(
        self,
        messages,
        **kwargs: Any,
    ) -> str:
        request = kwargs
        request["messages"] = messages
        request.update(self._default_params)
        response = self.client.post(request)
        return response['choices'][0]['message']['content'].rstrip()
    

if __name__ == "__main__":
    ChatGLM = ChatGLM()
    response = ChatGLM.generate("你好！请问你是？")
    print(response)


